# Large Language Models (LLMs)

# Overview
Large Language Models (LLMs) are deep neural networks in the domain of applied natural language processing (NLP), which are capable of understanding, generating and interpreting human language. The success of LLMs is related to the transformer architecture allowing the model to capture a wide variety of linguistic nuances, contexts, and patterns. 

Typically, LLMs are trained on vast quantities of text data and also consist of a large set of parameters in the range of hundres of billions parameters in total. Those parameters are the adjustable weights in the network that are optimized during the training in order to predict the next word in a sequence.


# References

- Sebastian Raschka, *Build a Large Language Model (from Scratch)*, Sebastian Raschka, Manning Publications, 2024.
